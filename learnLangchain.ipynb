{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJEp6pOG8eTK"
      },
      "source": [
        "# Tutorial for learning langchain\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqLgn-cKncGY",
        "outputId": "c68d7812-1a4f-4485-d913-dcd1465cc4d5"
      },
      "outputs": [],
      "source": [
        "# checking up gpu\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDp7mOpD8z0o"
      },
      "source": [
        "Installation of libraries || set up env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7y9ZCsa8yts",
        "outputId": "fade81bc-6038-4dfc-fd60-bb321c3d0213"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openaikey')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTyLC78YAJKu"
      },
      "source": [
        "# Simple import of llm and giving prompt query to it\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKsaDyqaAJmF",
        "outputId": "d0145274-5097-4298-ddcf-e21da1ef76f7"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.5)\n",
        "name = llm(\"I want to open a coding club in the field of AI . suggest a creative name for it.\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnviPWgGDNUl"
      },
      "source": [
        "# Doing the same with prompt template which takes a input variable for the specific prompts\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mpHx12JmDbNL",
        "outputId": "1a560316-48be-4001-8137-3d5df480a0ec"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_tech = PromptTemplate(\n",
        "    input_variables=['tech'],\n",
        "    template=\"I want to open a coding club in the field of {tech} . suggest a creative name for it.\"\n",
        ")\n",
        "\n",
        "prompt_template_tech.format(tech=\"blockchain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2fsJy3yEb8k"
      },
      "source": [
        "# For utilizing this template we hava to use chain in which we can provide the llm and template for output.& Also use run method for giving the input to the prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VB3wu7B_FEdv",
        "outputId": "8eef7a97-b329-4e51-bad7-61ff8ebe19de"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_tech)\n",
        "chain.run(\"DSA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T7_FB8kB5O8"
      },
      "source": [
        "# Find every function of chain than :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tRPSHEGCCQh",
        "outputId": "7302f241-3217-4278-83d5-818b1869280b"
      },
      "outputs": [],
      "source": [
        "dir(chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0oJLHIkG_us"
      },
      "source": [
        "# Use of simple sequential chain\n",
        "combining of two prompt steps in which one's putput is second one's input\n",
        "\n",
        "For ex.\n",
        "\n",
        "input(tech)=blockchain -> CodeCrushers(generated name) -> now we want a sub team name into this club fro that i use this name as input -> geberate a sub tean names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5Hp5MUTMG7qg",
        "outputId": "919b1e0d-ee9b-4775-97b3-f3da82cce0c7"
      },
      "outputs": [],
      "source": [
        "# first prompt template we already mentioned\n",
        "prompt_template_tech = PromptTemplate(\n",
        "    input_variables=['tech'],\n",
        "    template=\"I want to open a coding club in the field of {tech} . suggest a creative name for it.\"\n",
        ")\n",
        "\n",
        "tech_chain = LLMChain(llm=llm, prompt=prompt_template_tech)\n",
        "\n",
        "# second template\n",
        "prompt_template_team =  PromptTemplate(\n",
        "    input_variables=['club_name'],\n",
        "    template=\"Suggest me a sub team names for coding club : {club_name} . return a number wise output for it.\"\n",
        ")\n",
        "\n",
        "team_chain = LLMChain(llm=llm, prompt=prompt_template_team)\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "chain = SimpleSequentialChain(chains = [tech_chain,team_chain])\n",
        "\n",
        "chain.run(\"DSA\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqU9l2XkMy8y"
      },
      "source": [
        "# Use of sequential chain\n",
        "but what if i want both fist and second one's  output than :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acPtbCrvI0cF",
        "outputId": "b27ff6aa-5278-49b1-e1aa-01adc624c86d"
      },
      "outputs": [],
      "source": [
        "# do the same thing we done in sompleseqchain for prompt\n",
        "# first prompt template we already mentioned\n",
        "prompt_template_tech = PromptTemplate(\n",
        "    input_variables=['tech'],\n",
        "    template=\"I want to open a coding club in the field of {tech} . suggest a creative name for it.\"\n",
        ")\n",
        "\n",
        "tech_chain = LLMChain(llm=llm, prompt=prompt_template_tech, output_key='club_name')\n",
        "\n",
        "# second template\n",
        "prompt_template_team =  PromptTemplate(\n",
        "    input_variables=['club_name'],\n",
        "    template=\"Suggest me a sub team names for coding club : {club_name} . return a number wise output for it.\"\n",
        ")\n",
        "\n",
        "team_chain = LLMChain(llm=llm, prompt=prompt_template_team,output_key='team_name')\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [tech_chain,team_chain],\n",
        "    input_variables = ['tech'], # we have multiple input var,\n",
        "    output_variables = ['club_name','team_name']\n",
        "    )\n",
        "\n",
        "chain({'tech' : 'App Development'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_EO96ciiAY"
      },
      "source": [
        "# Creating Agent using llms\n",
        "\n",
        "*   Agent is kind of mixture of one or more service that retrieves data and help the resoning part of llm to figure out answer\n",
        "*   For Example :\n",
        "    * We have to calculate that how many years are completed of independence day in that two service are called\n",
        "    1) retrives data from wikipedia and find year of independence\n",
        "    2) math function that calculate the current year - retrieved year.\n",
        "    * Here it called a agent which do both of the work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "L4PBDA9o3ei8",
        "outputId": "bc2e87d9-0760-4089-c844-3ea048e385de"
      },
      "outputs": [],
      "source": [
        "!pip install wikipedia\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "tools = load_tools([\"wikipedia\",\"llm-math\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "   # verbose is only for description of work how the diff steps work\n",
        "    verbose=True\n",
        ")\n",
        "print(\"----------  OUTPUT STARTS FROM HERE ----------\")\n",
        "agent.run(\"How many years are completed from independence of india? \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVXxemb49pqX"
      },
      "source": [
        "# Let's take a hand on serp api which is simple google search and give answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "DJskaW0Z97BQ",
        "outputId": "c16e98aa-65d5-47c0-c90f-0505d38df1ed"
      },
      "outputs": [],
      "source": [
        "!pip install google-search-results\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['SERPAPI_API_KEY'] = userdata.get('serpkey')\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "tools = load_tools([\"serpapi\",\"llm-math\"],llm=llm)\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        "    )\n",
        "\n",
        "agent.run(\"first find What is GDP per capita of india and do plus 2? \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaLQHsC0DFhu"
      },
      "source": [
        "# Memory\n",
        "  # When we do the chat with chain don't have state to remember last prompts and data.\n",
        "  # for that chain.memory is used you can show that by type(chain.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6upNgWaDf3R",
        "outputId": "849d22bb-a957-49c2-f656-1110dcd0464c"
      },
      "outputs": [],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHVPZiQuDr9u",
        "outputId": "ae34f98a-1acf-4e29-c8a0-564b24ebb93a"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# initializing it\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_tech, memory=memory)\n",
        "name = chain.run(\"AI\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8hNOQW0EeZR",
        "outputId": "c8365b45-b731-46f1-96a5-7a8f4c3afc85"
      },
      "outputs": [],
      "source": [
        "name = chain.run(\"Blockchain\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXZ3KzdYE1Xc"
      },
      "source": [
        "Now show the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klvVOPSZEu-X",
        "outputId": "72da7293-269c-4fb1-9deb-1e1a455b4e2c"
      },
      "outputs": [],
      "source": [
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6NHGULZFDNV",
        "outputId": "8048544d-2f86-4696-a6a1-6f6491e1027d"
      },
      "outputs": [],
      "source": [
        "print(chain.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8cTkcdSG-0I"
      },
      "source": [
        "# Conversational Chain which stoes all previous chats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKeKxOx8Gkz8",
        "outputId": "39cd74b6-3ea3-489d-ee9f-fee49f952931"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "converse = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(converse.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0dfkUsBtHMjy",
        "outputId": "5b24b415-da5d-432a-dcb0-1f8a8232a24d"
      },
      "outputs": [],
      "source": [
        "converse.run(\"Who won the fifa 2020?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "YPldivPiHbuC",
        "outputId": "ee804901-9b6a-4288-eba5-b9afdacb8b4d"
      },
      "outputs": [],
      "source": [
        "converse.run(\"Who is the caption of winnig team?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "biIuHhYSHm8A",
        "outputId": "a63b7038-9963-49e0-f9bf-151ba68634f6"
      },
      "outputs": [],
      "source": [
        "converse.memory.buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PMk5bmIH4gN"
      },
      "source": [
        "we have only store last x memory than"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs8PqdLuH-NS"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1) #k is the value of how many we have to send or store in buffer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
